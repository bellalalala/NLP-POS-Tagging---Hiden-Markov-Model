{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "### 4.1 \n",
    "#### Report word-tag counts, tag unigram counts, and tag bigram counts. Let’s denote these by C (wi ,ti), C (ti), and C (ti−1 ,ti) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files and remove space lines\n",
    "import glob\n",
    "read_files = glob.glob(\"C:\\\\Users\\\\81468\\\\OneDrive\\\\Documents\\\\CS6120\\\\homework\\\\a1\\\\a1_4\\\\brown\\\\*\")\n",
    "train = []\n",
    "for i in range(len(read_files)):\n",
    "    with open(read_files[i], \"rb\") as infile:\n",
    "        for line in infile:\n",
    "            if not line.isspace():\n",
    "                train.append(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <s>/START <e>/END, replace duplicate spaces and \\n to single space\n",
    "import re\n",
    "for i in range(len(train)):\n",
    "        train[i] = '<s>/START %s <e>/END' %train[i]\n",
    "        train[i] = re.sub(r'[\\s]',' ',train[i]) \n",
    "        train[i] = re.sub(r' {2,}',' ',train[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = ' '.join(train) #whole string\n",
    "train_wt_l = train_raw.split(' ') #list of word_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(s):\n",
    "    c=dict.fromkeys(set(s),0)\n",
    "    for i in range(0,len(s)):\n",
    "        c[s[i]] = c[s[i]]+1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = [] #list of words\n",
    "for i in range(len(train_wt_l)):\n",
    "    word.append(train_wt_l[i].rsplit('/', 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_c = count(word) #count distinct word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace words occuring less than 5 times with UNK in word list\n",
    "word_rep = []\n",
    "for i in range(len(word)):\n",
    "    if word_c[word[i]] <= 5:\n",
    "        word_rep.append('<UNK>')\n",
    "    else:\n",
    "        word_rep.append(word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace words occuring less than 5 times with UNK in word_tag list\n",
    "word_tag_rep = []\n",
    "for i in range(len(train_wt_l)):\n",
    "    if word_c[word[i]] <= 5:\n",
    "        word_tag_rep.append('<UNK>/%s' % train_wt_l[i].rsplit('/', 1)[1])\n",
    "    else:\n",
    "        word_tag_rep.append(train_wt_l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count replace word_tag list\n",
    "word_tag_rep_count = count(word_tag_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count replaced word list\n",
    "word_rep_count = count(word_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = [] #list of tags\n",
    "for i in range(len(train_wt_l)):\n",
    "    tag.append(train_wt_l[i].rsplit('/', 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count tag\n",
    "tag_count = count(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count bi_tag{'tag1 tag2':c1, 'tag2 tag3':c2,...}\n",
    "bi_tag = []\n",
    "for i in range(len(tag)-1):\n",
    "    bi_tag.append(tag[i]+' '+tag[i+1])\n",
    "bi_tag_count = count(bi_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report files\n",
    "with open('word_tag_counts.txt', 'w') as f:\n",
    "    for key, value in word_tag_rep_count.items():\n",
    "        f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "with open('tag_counts.txt', 'w') as f:\n",
    "    for key, value in tag_count.items():\n",
    "        f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "with open('bi_tag_counts.txt', 'w') as f:\n",
    "    for key, value in bi_tag_count.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 \n",
    "####  transition probabilities of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(tag_count)\n",
    "uniq = list(set(tag_count.keys()))\n",
    "prob_tag_smooth = {}\n",
    "for i in range(V):\n",
    "    for j in range(V):\n",
    "        k2 = uniq[i]+' '+uniq[j]\n",
    "        if k2 in bi_tag_count:\n",
    "            prob_tag_smooth[k2] = (bi_tag_count[k2]+0.1)/(tag_count[uniq[i]]+0.1*V)\n",
    "        else:\n",
    "            prob_tag_smooth[k2] = 0.1/(tag_count[uniq[i]] + 0.1*V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bi_tag_prob.txt', 'w') as f:\n",
    "    for key, value in prob_tag_smooth.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothed transition probability in format {tag1:{{tag1:p1},{tag2,p2},...},tag2:{{tag1:p3},{tag2:p4},...},...}\n",
    "transition = {}\n",
    "for t in tag_count:\n",
    "    transition[t] = {}\n",
    "    for pt in prob_tag_smooth:\n",
    "        if pt.split(' ')[0] == t:\n",
    "            transition[t][pt.split(' ')[1]] = prob_tag_smooth[pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsmoothed transition probability\n",
    "prob_tag = {}\n",
    "for k2 in bi_tag_count:\n",
    "            prob_tag[k2] = bi_tag_count[k2]/tag_count[k2.split(' ')[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3\n",
    "#### emission probabilities of the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the format to {tag1:{word1:count1,word2:count2,...},tag2:{word1:count1,word3:count3,...},...}\n",
    "word_tag_rep_count_2 = dict.fromkeys(tag_count,{})\n",
    "for t in word_tag_rep_count_2:\n",
    "    word_tag_rep_count_2[t]={}\n",
    "    for wt in word_tag_rep_count.keys():\n",
    "        if wt.rsplit('/',1)[1] == t:\n",
    "            word_tag_rep_count_2[t][wt.rsplit('/',1)[0]] = word_tag_rep_count[wt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothed emission probability: in format {tag1:{word1:p1,word2:p2,...},tag2:{word1:p1,word3:p,...},...}\n",
    "V_w = len(word_rep_count) #number of unique words (including <UNK>)\n",
    "uniq_w = list(set(word_rep_count.keys())) #list of unique words (including <UNK>)\n",
    "emission = {}\n",
    "for t in word_tag_rep_count_2:\n",
    "    emission[t] = {}\n",
    "    for w in uniq_w:\n",
    "        if w in word_tag_rep_count_2[t]:\n",
    "            emission[t][w]=(word_tag_rep_count_2[t][w]+0.1)/(tag_count[t]+0.1*V_w)\n",
    "        else:\n",
    "            emission[t][w]= 0.1/(tag_count[t]+0.1*V_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a different version for report\n",
    "prob_word_tag_smoothed = {}\n",
    "for t in emission:\n",
    "    for w in emission[t]:\n",
    "        prob_word_tag_smoothed['%s/%s' %(w,t)] = emission[t][w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report smoothed one\n",
    "with open('word_tag_prob.txt', 'w') as f:\n",
    "    for key, value in prob_word_tag_smoothed.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsmoothed prob_word_tag\n",
    "prob_word_tag = {}\n",
    "for wt in word_tag_rep_count:\n",
    "    prob_word_tag[wt] = word_tag_rep_count[wt]/tag_count[wt.rsplit('/',1)[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "#### Generate 5 random sentences using the previously learned HMM. Output each sentence (with the POS tags) and its probability of being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S_1> on till me is ' fear Bridge act Water out up Success account -- was yours '' <UNK> You Belgians believing dark <UNK> you the -- it No nineteenth Liberal Set reassuring The Greater Garibaldi's Americans If : ?\n",
      "Prob = 3.1870591164904557e-163\n",
      "<S_2> I'd been away .\n",
      "Prob = 8.900890826495666e-10\n",
      "<S_3> They'd easier : <UNK> injustice Though <UNK> depending does <UNK> <UNK> accruing Major State's `` Americans or President's <UNK> Republic each Sunday's 700 , arms ( John , more to be lighter ease out , trusts toward ) ? :\n",
      "Prob = 5.027651129293066e-146\n",
      "<S_4> downtown : Berlin or how The International , may second student could be better !\n",
      "Prob = 5.0802085824224045e-43\n",
      "<S_5> Ill. )\n",
      "Prob = 7.958515235021437e-07\n"
     ]
    }
   ],
   "source": [
    "#generate sentences using unsmoothed emission probability and unsmoothed transition probability\n",
    "import random\n",
    "from functools import reduce\n",
    "for k in range(0,5):\n",
    "    sentence_tag =[]\n",
    "    tag_p = []\n",
    "    w = {} \n",
    "    s = 'START'\n",
    "    while s != 'END':\n",
    "        for bt in prob_tag:\n",
    "            if bt.split(' ')[0] == s:\n",
    "                w[bt] = prob_tag[bt]\n",
    "        v = random.choice(list(w.items()))\n",
    "        sentence_tag.append(v[0])\n",
    "        tag_p.append(v[1])\n",
    "        w = {} \n",
    "        s = sentence_tag[-1].split(' ')[1]\n",
    "    w_2 = {}\n",
    "    sentence_w = []\n",
    "    word_p = []\n",
    "    for i in range(len(sentence_tag)-1):\n",
    "        for wt in prob_word_tag:\n",
    "            if wt.rsplit('/',1)[1] == sentence_tag[i].split(' ')[1]:\n",
    "                w_2[wt] = prob_word_tag[wt]\n",
    "        v = random.choice(list(w_2.items()))\n",
    "        sentence_w.append(v[0].rsplit('/',1)[0])\n",
    "        word_p.append(v[1])\n",
    "        w_2 = {}\n",
    "    sentence = ' '.join(sentence_w)\n",
    "    p = reduce(lambda x, y: x*y, word_p) * reduce(lambda x, y: x*y, tag_p)\n",
    "    print('<S_%s> %s' %(k+1,sentence))\n",
    "    print('Prob = %s'% p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5\n",
    "#### For each word in the test dataset, derive the most probable POS tag sequence using the Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test file and combine sentences into a list \n",
    "with open('Test_File.txt', \"rb\") as infile:\n",
    "    data = infile.read().decode('utf-8').split('<EOS>')\n",
    "for i in range(len(data)):\n",
    "    data[i] = list(filter(None, data[i].split('\\r\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Viterbi algorithm to calculate the best path(most possible POS tag sequence)\n",
    "bestpath = {}\n",
    "bestpathprob = {}\n",
    "bestpathpointer = {}\n",
    "for i in range(len(data)-1):\n",
    "    T = len(data[i])\n",
    "    #initialize viterbi{N:{T:}}\n",
    "    t = 1\n",
    "    viterbi = {}\n",
    "    backpointer = {}\n",
    "    for s in tag_count:\n",
    "        viterbi[s] = {}\n",
    "        backpointer[s] = {}\n",
    "        if data[i][t] in word_rep_count:\n",
    "            viterbi[s][data[i][t]] = transition['START'][s] * emission[s][data[i][t]]\n",
    "        else:\n",
    "            viterbi[s][data[i][t]] = transition['START'][s] * emission[s]['<UNK>']\n",
    "        backpointer[s][data[i][t]] = 0\n",
    "    for t in range(2,T):\n",
    "        for s in tag_count:\n",
    "            viterbi[s][data[i][t]] = 0\n",
    "            for s_0 in tag_count:\n",
    "                if data[i][t] in word_rep_count:\n",
    "                    new_p = viterbi[s_0][data[i][t-1]] * transition[s_0][s] * emission[s][data[i][t]]\n",
    "                else:\n",
    "                    new_p = viterbi[s_0][data[i][t-1]] * transition[s_0][s] * emission[s]['<UNK>']\n",
    "                viterbi[s][data[i][t]]= max(viterbi[s][data[i][t]], new_p)\n",
    "                if viterbi[s][data[i][t]] == new_p:\n",
    "                    backpointer[s][data[i][t-1]] = s_0\n",
    "    bestpathprob[data[i][0]] = 0\n",
    "    for s in tag_count:\n",
    "        bestpathprob[data[i][0]] = max(bestpathprob[data[i][0]], viterbi[s][data[i][T-1]])\n",
    "        if bestpathprob[data[i][0]] == viterbi[s][data[i][T-1]]:\n",
    "            bestpathpointer[data[i][0]] = s\n",
    "        backpointer[s][data[i][T-1]] = bestpathpointer[data[i][0]]\n",
    "    bestpath[data[i][0]] = backpointer[bestpathpointer[data[i][0]]]\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report best path\n",
    "with open('POS_tag_result.txt', 'w') as f:\n",
    "    for key in bestpath:\n",
    "        f.write('%s\\n' % key)\n",
    "        for nest_key, value in bestpath[key].items():\n",
    "            f.write('%s, %s\\n' % (nest_key, value))\n",
    "        f.write('<EOS>\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
